{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NDSI for Switzerland with Landsat 5,7,8 and Sentinel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pour vider la poubelle du sdc\n",
    "!rm -rf ~/.local/share/Trash/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to resolve driver datacube.plugins.index::default\n",
      "Error was: ContextualVersionConflict(click 8.0.4 (/home/localuser/Datacube/datacube_env/lib/python3.6/site-packages), Requirement.parse('click<8,>=4.0'), {'rasterio', 'cligj'})\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import calendar\n",
    "# import shutil\n",
    "import gdal\n",
    "import osr\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#to create DEM classes\n",
    "from xrspatial.classify import equal_interval\n",
    "from datashader.transfer_functions import shade\n",
    "from xrspatial.classify import reclassify\n",
    "\n",
    "from datetime import datetime\n",
    "# from multiprocessing import Pool, Lock, Manager\n",
    "from matplotlib import colors\n",
    "from IPython.display import display, Javascript\n",
    "from time import time\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from collections import Counter\n",
    "import pandas as pd #for data analysing\n",
    "\n",
    "import datacube\n",
    "dc = datacube.Datacube()\n",
    "\n",
    "from utils.data_cube_utilities.dc_display_map import display_map\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import draw_map\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import create_slc_clean_mask, ls_qa_clean\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import load_multi_clean_30, load_lss2_clean_30, mix_dataset\n",
    "\n",
    "\n",
    "from utils.data_cube_utilities.dc_display_map import display_map, _degree_to_zoom_level\n",
    "from utils.data_cube_utilities.dc_chunker import create_geographic_chunks, combine_geographic_chunks\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import printandlog, load_multi_clean\n",
    "from swiss_utils.data_cube_utilities.sdc_utilities import write_geotiff_from_xr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration section\n",
    "\n",
    "products = [\"ls7_ledaps_swiss\",\"ls5_ledaps_swiss\",'ls8_lasrc_swiss', 's2_l2a_10m_swiss']\n",
    "platforms = [\"LANDSAT_7\",\"LANDSAT_5\",'LANDSAT_8','SENTINEL_2']\n",
    "\n",
    "measurements=['green','swir1','pixel_qa','slc]\n",
    "\n",
    "\n",
    "############### Switzerland all #############\n",
    "min_lon =  5.7\n",
    "max_lon =  10.55\n",
    "min_lat = 45.74\n",
    "max_lat = 47.85\n",
    "\n",
    "\n",
    "#start_date = datetime.strptime(\"1984-11-30\", '%Y-%m-%d')\n",
    "#end_date = datetime.strptime(\"2000-01-01\", '%Y-%m-%d')\n",
    "\n",
    "\n",
    "start_year =1984\n",
    "end_year = 2000\n",
    "\n",
    "start_month = 12\n",
    "end_month = 12\n",
    "\n",
    "ind_season = ['DJF','DJF',\n",
    "              'MAM','MAM','MAM',\n",
    "              'JJA','JJA','JJA',\n",
    "              'SON','SON','SON',\n",
    "              'DJF']\n",
    "\n",
    "resolution=(0.0003407435000010011,0.0003407435000010325)\n",
    "resampling={'*': 'cubic', 'slc': 'nearest'}\n",
    "\n",
    "work_path = './really_bigdata/Test_CP'\n",
    "log_name = 'test_cp.log'\n",
    "user_mail = 'cpoussin0@gmail.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><iframe src=\"data:text/html;charset=utf-8;base64,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\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x7f4a6caa1d30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_map(latitude = (min_lat, max_lat), longitude = (min_lon, max_lon))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask de la suisse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers \n",
      "==== Some info ====\n",
      "Size of tif file is 14234 x 6194 x 1\n",
      "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]]\n",
      "The shape of the array is (6194, 14234) \n"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "dataset = gdal.Open('Charlotte/Mask_Suisse/Mask_Suisse.tif')\n",
    "print(\"Number of layers \".format(dataset.RasterCount))\n",
    "print(\"==== Some info ====\")\n",
    "print(\"Size of tif file is {} x {} x {}\".format(dataset.RasterXSize,\n",
    "                                    dataset.RasterYSize,\n",
    "                                    dataset.RasterCount))\n",
    "print(\"Projection is {}\".format(dataset.GetProjection()))\n",
    "Mask_Suisse = np.array(dataset.GetRasterBand(1).ReadAsArray())\n",
    "print(\"The shape of the array is {} \".format(Mask_Suisse.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mask_Suisse.resize(6195, 14234) # pour landsat seul\n",
    "#Mask_Suisse.resize(6199, 14234) #pour landsat et sentinel ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(6195, 14234)'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(Mask_Suisse.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4963e1ba90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Mask_Suisse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask lacs \n",
    "Attention pour les lacs c'est 0 et pas 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers \n",
      "==== Some info ====\n",
      "Size of tif file is 14234 x 6194 x 1\n",
      "Projection is GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0],UNIT[\"degree\",0.0174532925199433],AUTHORITY[\"EPSG\",\"4326\"]]\n",
      "The shape of the array is (6194, 14234) \n"
     ]
    }
   ],
   "source": [
    "from osgeo import gdal\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "dataset = gdal.Open('Charlotte/Mask_lacs/Mask_lacs.tif')\n",
    "print(\"Number of layers \".format(dataset.RasterCount))\n",
    "print(\"==== Some info ====\")\n",
    "print(\"Size of tif file is {} x {} x {}\".format(dataset.RasterXSize,\n",
    "                                    dataset.RasterYSize,\n",
    "                                    dataset.RasterCount))\n",
    "print(\"Projection is {}\".format(dataset.GetProjection()))\n",
    "Mask_lacs = np.array(dataset.GetRasterBand(1).ReadAsArray())\n",
    "print(\"The shape of the array is {} \".format(Mask_lacs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mask_lacs.resize(6195, 14234) #pour landsat seul\n",
    "#Mask_lacs.resize(6199, 14234) #pour landsat et sentinel ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(6195, 14234)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format(Mask_lacs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f495e4d5550>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(Mask_lacs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions pour le mix Landsat-sentinel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_platform(dc, products):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Create a list of platforms from a list of products\n",
    "    Input:\n",
    "      dc:       datacube.api.core.Datacube\n",
    "                The Datacube instance to load data with.\n",
    "      products: list of products\n",
    "    Output:\n",
    "      list of platforms\n",
    "    Authors:\n",
    "      Bruno Chatenoux (UNEP/GRID-Geneva, 4.3.2019)\n",
    "    \"\"\"\n",
    "    list_of_products = dc.list_products()\n",
    "    platforms = []\n",
    "    for product in products:\n",
    "        try:\n",
    "            platforms.append(list_of_products[list_of_products['name'] == product].iloc[0]['platform'])\n",
    "        except:\n",
    "            sys.exit('Cannot find a platform for product \\\"%s\\\"' % (prod))\n",
    "    return platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multi_clean_30(dc, products, time, lon, lat, measurements,resolution, resampling, dropna = True, platforms = [], valid_cats = []):\n",
    "    ##pour loader à 30 mèetre de résolution les donées sentinel-2 avec le masque\n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Create a clean dataset (multi-product or not) using cleaning \"autor's recommended ways\"\n",
    "      - ls_qa_clean\n",
    "      - create_slc_clean_mask\n",
    "      Sorted by ascending time\n",
    "      Works with Landsat or Sentinel 2 (but not mixed).\n",
    "      Platforms arguments are not mandatory\n",
    "      dropna option removes time without any data\n",
    "    Input:\n",
    "      dc:           datacube.api.core.Datacube\n",
    "                    The Datacube instance to load data with.\n",
    "    Args:\n",
    "      platforms:    list of platforms (not mandatory)\n",
    "      products:     list of products\n",
    "      time:         pair (list) of minimum and maximum date\n",
    "      lon:          pair (list) of minimum and maximum longitude\n",
    "      lat:          pair (list) of minimum and maximum longitude\n",
    "      measurements: list of measurements (must include pixel_qa or slc (not and !))\n",
    "      dropna:       if True removes times without any data\n",
    "      valid_cats:   array of ints representing what category should be considered valid\n",
    "                    * meand category by default\n",
    "      # SENTINEL 2 ################################\n",
    "      #   0 - no data                             #\n",
    "      #   1 - saturated or defective              #\n",
    "      #   2 - dark area pixels                    #\n",
    "      #   3 - cloud_shadows                       #\n",
    "      #   4 * vegetation                          #\n",
    "      #   5 * not vegetated                       #\n",
    "      #   6 * water                               #\n",
    "      #   7 * unclassified                        #\n",
    "      #   8 - cloud medium probability            #\n",
    "      #   9 - cloud high probability              #\n",
    "      #  10 - thin cirrus                         #\n",
    "      #  11 * snow                                #\n",
    "      #############################################\n",
    "      # LANDSAT 5, 7 and 8 ########################\n",
    "      #    0 : Fill                               #\n",
    "      #    1 * Clear                              #\n",
    "      #    2 * Water                              #\n",
    "      #    3 : Cloud shadow                       #\n",
    "      #    4 * Snow                               #\n",
    "      #    5 : Cloud                              #\n",
    "      #   10 : Terrain occlusion (Landsat 8 only) #\n",
    "      #############################################\n",
    "    Output:\n",
    "      cleaned dataset and clean_mask sorted by ascending time\n",
    "    Authors:\n",
    "      Bruno Chatenoux (UNEP/GRID-Geneva, 10.12.2019)\n",
    "    \"\"\"\n",
    "\n",
    "    # Check submitted input\n",
    "    # Convert product string into list\n",
    "    if isinstance(products, str):\n",
    "        products = products.split()\n",
    "    # Get platforms if not provided\n",
    "    if len(products) != len(platforms):\n",
    "        platforms = get_platform(dc, products)\n",
    "    # Check LANDSAT and SENTINEL products are not mixed using products prefix\n",
    "    prfx = []\n",
    "    for platform in platforms:\n",
    "        prfx. append(platform.split('_')[0])\n",
    "    if len(set(prfx)) > 1:\n",
    "        sys.exit('Mixed platforms %s' % (set(prfx)))\n",
    "\n",
    "    # Create raw dataset\n",
    "    dataset_clean = None\n",
    "    for product,platform in zip(products, platforms):\n",
    "        dataset_tmp = dc.load(platform = platform, product = product,\n",
    "                         time = time,\n",
    "                         lon = lon,\n",
    "                         lat = lat,\n",
    "                         resolution = resolution,\n",
    "                         resampling = resampling,\n",
    "                         measurements = measurements)\n",
    "\n",
    "        if len(dataset_tmp.variables) == 0: continue # skip the current iteration if empty\n",
    "\n",
    "        # Clean dataset_tmp\n",
    "        if prfx[0] == \"LANDSAT\":\n",
    "            if len(valid_cats) == 0: valid_cats = [1, 2, 4]\n",
    "            clean_mask_tmp = ls_qa_clean(dataset_tmp.pixel_qa, valid_cats)\n",
    "        elif platforms[0] == \"SENTINEL_2\":\n",
    "            if len(valid_cats) == 0: valid_cats = [4, 5, 6, 7, 11]\n",
    "            clean_mask_tmp = create_slc_clean_mask(dataset_tmp.slc, valid_cats)\n",
    "        dataset_clean_tmp = dataset_tmp.where(clean_mask_tmp)\n",
    "        del dataset_tmp\n",
    "\n",
    "        # Remove negative values\n",
    "        dataset_clean_tmp = dataset_clean_tmp.where(dataset_clean_tmp >= 0)\n",
    "        \n",
    "        if dataset_clean is None:\n",
    "            dataset_clean = dataset_clean_tmp.copy(deep=True)\n",
    "        else:\n",
    "            dataset_clean = xr.concat([dataset_clean, dataset_clean_tmp], dim = 'time')\n",
    "        del dataset_clean_tmp\n",
    "    \n",
    "    \n",
    "    #if dropna:\n",
    "        # remove time without any data\n",
    "        #dataset_clean = dataset_clean.dropna('time', how='all')\n",
    "    \n",
    "    if dataset_clean is not None:\n",
    "        # Sort dataset by ascending time\n",
    "        dataset_clean = dataset_clean.sortby('time')\n",
    "        return (dataset_clean, ~np.isnan(dataset_clean[measurements[0]].values))\n",
    "    else:\n",
    "        return (0, 0)\n",
    "\n",
    "def tile_array(a, x0, x1, x2):\n",
    "    t, r, c = a.shape                                    # number of rows/columns\n",
    "    ts, rs, cs = a.strides                                # row/column strides \n",
    "    x = as_strided(a, (t, x0, r, x1, c, x2), (ts, 0, rs, 0, cs, 0)) # view a as larger 4D array\n",
    "    return x.reshape(t*x0, r*x1, c*x2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lss2_clean_30(dc, products, time, lon, lat, measurements,\n",
    "                    dropna = False, platforms = [], valid_cats = [[],[]]):\n",
    "    ##pour loader dans un dictionnaire les données landsat et sentinel-2 à 30 mêtre de resolution \n",
    "    \"\"\"\n",
    "    Description:\n",
    "      Create a clean dataset mixing Landsat and Sentinel 2 products (respectively with prefixs 'ls' and 's2')\n",
    "      and using cleaning \"autor's recommended ways\":\n",
    "      - ls_qa_clean\n",
    "      - create_slc_clean_mask\n",
    "      Sorted by ascending time\n",
    "      If resample option is activated ('up' or 'down_mean', 'down_median') up/downsampling is performed and\n",
    "      products output combined into a single 'lss2' prefix\n",
    "      dropna option removes time without any data   \n",
    "      This function works as load_multi_clean function, but with a mix of Landsat and Sentinel 2 products\n",
    "      the resampl option was added (to optionally combine products output), and platforms options not used\n",
    "\n",
    "    Input:\n",
    "      dc:           datacube.api.core.Datacube\n",
    "                    The Datacube instance to load data with.\n",
    "    Args:\n",
    "      products:     list of products\n",
    "      time:         pair (list) of minimum and maximum date\n",
    "      lon:          pair (list) of minimum and maximum longitude\n",
    "      lat:          pair (list) of minimum and maximum longitude\n",
    "      measurements: list of measurements (without mask band, landsat and Sentinel 2 products prefix shouls be\n",
    "                    'ls or 's2)\n",
    "      resampl:      (OPTIONAL) Up/Downsample ('up', 'down_mean', 'down_median' ) products and combine their\n",
    "                    output\n",
    "      dropna:       (OPTIONAL) if True removes times without any data\n",
    "      platforms:    (OPTIONAL) list of platforms (not used but kept to better mimic load_multi_clean function)\n",
    "      valid_cats:   (OPTIONAL) list of list of ints representing what category should be considered valid\n",
    "                    first Landsat categories, then Sentinel 2 categories\n",
    "                    * meand category by default\n",
    "      # SENTINEL 2 ################################\n",
    "      #   0 - no data                             #\n",
    "      #   1 - saturated or defective              #\n",
    "      #   2 - dark area pixels                    #\n",
    "      #   3 - cloud_shadows                       #\n",
    "      #   4 * vegetation                          #\n",
    "      #   5 * not vegetated                       #\n",
    "      #   6 * water                               #\n",
    "      #   7 * unclassified                        #\n",
    "      #   8 - cloud medium probability            #\n",
    "      #   9 - cloud high probability              #\n",
    "      #  10 - thin cirrus                         #\n",
    "      #  11 * snow                                #\n",
    "      #############################################\n",
    "      # LANDSAT 5, 7 and 8 ########################\n",
    "      #    0 : Fill                               #\n",
    "      #    1 * Clear                              #\n",
    "      #    2 * Water                              #\n",
    "      #    3 : Cloud shadow                       #\n",
    "      #    4 * Snow                               #\n",
    "      #    5 : Cloud                              #\n",
    "      #   10 : Terrain occlusion (Landsat 8 only) #\n",
    "      #############################################\n",
    "    Output:\n",
    "      cleaned dataset and clean_mask sorted by ascending time stored in dictionnaries,\n",
    "      if no up/downsampling is performed dictionnaries contains the two Landsat and Sentinel 2 output products\n",
    "    Authors:\n",
    "      Bruno Chatenoux (UNEP/GRID-Geneva, 11.12.2019)\n",
    "    \"\"\"\n",
    "    \n",
    "    # dictionnary sensor - mask band (Higher resolution first !)\n",
    "    dict_sensmask = {'ls':'pixel_qa',\n",
    "                     's2': 'slc'}\n",
    "    \n",
    "    resampl_opts = ['up']\n",
    "    \n",
    "    sensors = []\n",
    "    for product in products:\n",
    "        if product[:2] not in sensors:\n",
    "            sensors.append(product[:2])\n",
    "            \n",
    "    # check sensors\n",
    "    assert (sorted(set(sensors)) == sorted(set(dict_sensmask.keys()))), \\\n",
    "           '\\nA mix of Landsat and Sentinel 2 products is required !\\nYou should use load_multi_clean function'\n",
    "    \n",
    "    assert (len(valid_cats) == 2), \\\n",
    "           '\\nvalid_cats argument must be a list of list (read the doc for more details)'\n",
    "    \n",
    "    dict_dsc = {}\n",
    "    dict_cm = {}\n",
    "    \n",
    "    # Process first Landsat and then Sentinel 2 (based on dict_sensmask order)\n",
    "    for index, sensor in enumerate(dict_sensmask.keys()):\n",
    "        # Remove mask bands if any\n",
    "        measurements = [ele for ele in measurements if ele not in dict_sensmask.values()]\n",
    "        # append propermask band to measurements\n",
    "        measurements.append(dict_sensmask[sensor])\n",
    "        \n",
    "        # fix Sentinel 2 geographical extent based on Landsat dataset\n",
    "        if index == 1:\n",
    "            resx = (dsc.longitude.values.max() - dsc.longitude.values.min()) / len(dsc.longitude.values)\n",
    "            resy = (dsc.latitude.values.max() - dsc.latitude.values.min()) / len(dsc.latitude.values)\n",
    "            lon = (dsc.longitude.values.min() - resx / 3, dsc.longitude.values.max() + resx / 3)\n",
    "            lat = (dsc.latitude.values.min() - resy / 3, dsc.latitude.values.max() + resy / 3)\n",
    "        \n",
    "        dsc, cm = load_multi_clean_30(dc = dc,\n",
    "                                  products = [prod for prod in products if prod[:2] == sensor] ,\n",
    "                                  time = time,\n",
    "                                  lon = lon,\n",
    "                                  lat = lat,\n",
    "                                  measurements = measurements,\n",
    "                                  resolution = resolution,\n",
    "                                  resampling = resampling,\n",
    "                                  dropna = dropna,\n",
    "                                  valid_cats = valid_cats[index])\n",
    "        dict_dsc[sensor] = dsc\n",
    "        dict_cm[sensor] = cm\n",
    "    \n",
    "    return dict_dsc, dict_cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cloud mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate NDSI                               \n",
    "def NDSI(array):\n",
    "    return (array.green - array.swir1)/(array.green + array.swir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionnary of pixel_qa bit categories\n",
    "# Source: https://landsat.usgs.gov/landsat-surface-reflectance-quality-assessment\n",
    "\n",
    "bit_cats = {\"0\":\"Fill\",\n",
    "            \"1\":\"Clear\",\n",
    "            \"2\":\"Water\",\n",
    "            \"3\":\"Cloud shadow\",\n",
    "            \"4\":\"Snow\",\n",
    "            \"5\":\"Cloud\",\n",
    "            \"6\":\"Cloud Low confidence\",\n",
    "            \"7\":\"Cloud Medium confidence\",  # 6 and 7: \"Cloud High confidence\"\n",
    "            \"8\":\"Cirrus Low confidence\",    # LANDSAT 8 ONLY FROM HERE AND BELOW\n",
    "            \"9\":\"Cirrus Medium confidence\", # 8 and 9: \"Cirrus High confidence\"\n",
    "            \"10\":\"Terrain occlusion\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return bit length\n",
    "# Source: https://www.quora.com/How-can-I-get-the-bit-length-of-a-number-in-python\n",
    "def bit_length(int_type):\n",
    "    length = 0\n",
    "    while (int_type):\n",
    "        int_type >>= 1\n",
    "        length += 1\n",
    "    return(length)\n",
    "\n",
    "# Decode pixel_qa values\n",
    "def decode_qa(qa, dic):\n",
    "    cnt = 0\n",
    "    cats = []\n",
    "    for b in reversed(str(bin(qa))[2:]):\n",
    "        if b == '1':\n",
    "            # Create high conf category if bit 67 are 11\n",
    "            if cnt == 7 and bit_cats['6'] in cats:\n",
    "                cats.remove(dic[str(cnt - 1)])\n",
    "                cats.append('Cloud High confidence')\n",
    "            # Create high conf category if bit 89 are 11\n",
    "            if cnt == 9 and bit_cats['8'] in cats:\n",
    "                cats.remove(dic[str(cnt - 1)])\n",
    "                cats.append('Cirrus High confidence')\n",
    "            else:\n",
    "                cats.append(dic[str(cnt)])\n",
    "        cnt += 1\n",
    "    return cats\n",
    "\n",
    "# Print decoded pixel_qa values\n",
    "def print_qas(qas):\n",
    "    if len(qas) == 0:\n",
    "        print(\"No element to decode\")\n",
    "    for v in sorted(qas):\n",
    "        if v > 0:\n",
    "            cats = decode_qa(v, bit_cats)\n",
    "            print(\"%s: %s\" % (v, ', '.join(cats)))\n",
    "    return\n",
    "\n",
    "# Return unique values and count\n",
    "def unik_count(vals):\n",
    "    bc = vals.flatten()\n",
    "    bc = np.bincount(bc)\n",
    "    unik = np.nonzero(bc)[0]\n",
    "    cnt = bc[unik] * 100\n",
    "    return (unik, cnt)\n",
    "\n",
    "########################################\n",
    "##Define the clean mask cat. for all Landsat\n",
    "from functools import reduce  \n",
    "import numpy as np  \n",
    "\n",
    "def ls_qa_mask(ts, keys):\n",
    "    if bit_len == 8:\n",
    "        land_cover_endcoding = dict( fill          =  [1], \n",
    "                                     clear         =  [66,  130], \n",
    "                                     water         =  [68,  132],\n",
    "                                     shadow        =  [72,  136],\n",
    "                                     snow          =  [80,  112, 144, 176],\n",
    "                                     cloud         =  [96, 160, 224],\n",
    "                                     low_conf      =  [66,  68,  72, 96,],\n",
    "                                     med_conf      =  [130, 132, 136, 160,],\n",
    "                                     high_conf     =  [224],\n",
    "                                     low_conf_cl   =  [322, 324, 328, 352, 834, 836,     840, 864],#pour na pas avoir de message d'erreur\n",
    "                                   )\n",
    "    if bit_len >= 9:\n",
    "        land_cover_endcoding = dict( fill          =  [1] ,\n",
    "                                     clear         =  [66,  130, 322, 386, 834, 898, 1346],\n",
    "                                     water         =  [68,  132, 324, 388, 836, 900, 1348],\n",
    "                                     shadow        =  [72,  136, 328, 392, 840, 904, 1350],\n",
    "                                     snow          =  [80,  112, 144, 176, 336, 368, 400, 432, 848, 880, 812, 944, 1352],\n",
    "                                     cloud         =  [96,  112, 160, 176, 224, 352, 368, 416, 432, 848, 880, 912, 944, 1352],\n",
    "                                     low_conf      =  [66,  68,  72,  96,],\n",
    "                                     med_conf      =  [130, 132, 136, 160],\n",
    "                                     high_conf     =  [224],\n",
    "                                     low_conf_cl   =  [322, 324, 328, 352, 834, 836, 840, 864],\n",
    "                                     med_conf_cl   =  [386, 388, 392, 400, 416, 898, 900, 904, 928],\n",
    "                                     high_conf_cl  =  [480, 992],\n",
    "                                     low_conf_cir  =  [322, 324, 328, 336, 352, 368, 386, 388, 392, 400, 416, 432, 480],\n",
    "                                     high_conf_cir =  [834, 836, 840, 848, 864, 898, 900, 904, 912, 92], \n",
    "                                     terrain_occ   =  [1346,1348, 1350, 1352]\n",
    "                                   ) \n",
    "        \n",
    "    def merge_lists(a, b): \n",
    "        return a.union(set(land_cover_endcoding[b]))\n",
    "                        \n",
    "    relevant_encodings = reduce(merge_lists, keys,set())\n",
    "    return np.isin(ts.pixel_qa.values, list(relevant_encodings))\n",
    "    \n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-15 15:35:19.169976\n",
      "2024-04-15 15:35:19.170828 - 1/4\n",
      "2024-04-15 15:35:50.579557 - 2/4\n",
      "2024-04-15 15:36:52.860660 - 3/4\n",
      "2024-04-15 15:38:01.240640 - 4/4\n",
      "Done in 0:03:28.359582\n"
     ]
    }
   ],
   "source": [
    "# Create a multi-sensor dataset\n",
    "chunk_size = 3\n",
    "global_time_start = datetime.now()\n",
    "time_start = datetime.now()\n",
    "print(time_start)\n",
    "\n",
    "# Cut the geographic extents into chunks\n",
    "geographic_chunks = create_geographic_chunks(latitude=(min_lat, max_lat), longitude=(min_lon, max_lon), geographic_chunk_size=chunk_size)\n",
    "\n",
    "cnt = 0\n",
    "chunks_nb = len(geographic_chunks)\n",
    "product_chunks = []\n",
    "# Loop through chunks\n",
    "for index, chunk in enumerate(geographic_chunks):\n",
    "    cnt += 1\n",
    "    print('%s - %i/%i' % (datetime.now(), cnt, chunks_nb))\n",
    "    \n",
    "\n",
    "\n",
    "    create = True\n",
    "\n",
    "    for i in range(len(platforms)):\n",
    "        dataset_tmp = dc.load(platform = platforms[i],\n",
    "            product= products[i],\n",
    "            time=(start_date, end_date),\n",
    "            lon=chunk['longitude'], \n",
    "            lat=chunk['latitude'],\n",
    "            measurements=measurements)\n",
    "    \n",
    "        if len(dataset_tmp) > 0:\n",
    "            if create:\n",
    "                dataset_in = dataset_tmp.copy(deep=True)\n",
    "                create = False\n",
    "            else:\n",
    "                print(\"Append %i time\\n\" % (len(dataset_tmp.time)))\n",
    "                dataset_in = xr.concat([dataset_in, dataset_tmp], dim = 'time')\n",
    "\n",
    "    # Keep selected months\n",
    "    dataset_in = dataset_in.sel(time=np.isin(dataset_in['time.month'], sel_months))\n",
    "   \n",
    "    # Cloud mask\n",
    "    dc_qas, dc_cnt = unik_count(dataset_in.pixel_qa.values)\n",
    "    #print_qas(dc_qas)\n",
    "\n",
    "    bit_len = bit_length(max(dc_qas))\n",
    "    \n",
    "    ##Apply the mask in our dataset considering what we want\n",
    "    cl_mask = ls_qa_mask(dataset_in, [\"clear\",\"water\",\"snow\"])\n",
    "    ts_clean = dataset_in.where(cl_mask)\n",
    "    \n",
    "    #### NDSI ###\n",
    "    ndsi = NDSI(ts_clean)\n",
    "    #add variable NDSI in the dataset\n",
    "    ts_clean['ndsi']=ndsi\n",
    "    \n",
    "    ###Apply the region mask\n",
    "    ## To apply the mask in the dataset\n",
    "    #ts_clean_GranP = ts_clean.where((Mask_GranP ==1))\n",
    "    \n",
    "    ##### Max NDSI ######\n",
    "    #before to resample we have to sort time because of the multiplatform\n",
    "    #ndsi_sorted=ndsi.reindex(time=sorted(ndsi.time.values))\n",
    "    # resample by month instead of groupby\n",
    "    max_ndsi = ndsi.resample(time='1D', skipna=True).max('time')\n",
    "    # Remove empty dataset created by resample\n",
    "    max_ndsi = max_ndsi.dropna('time', how='all')\n",
    "    #create a dataset with the variable max_ndsi_month\n",
    "    dataset_clean_CH = max_ndsi.to_dataset(name='max_ndsi')\n",
    "\n",
    "    product_chunks.append(dataset_clean_CH)\n",
    "    \n",
    "# Combine chunks \n",
    "with np.errstate(divide='ignore'):\n",
    "    ts = combine_geographic_chunks(product_chunks)\n",
    "    \n",
    "del product_chunks\n",
    "del dataset_tmp\n",
    "del dataset_in\n",
    "\n",
    "print('Done in %s' % (datetime.now()-time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDSI = ts.where((Mask_lacs ==0))\n",
    "#NDSI = ts.where((Mask_Suisse ==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDSI = NDSI.where((Mask_Suisse ==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_geotiff_from_xr('really_bigdata/NDSI_l7_06.tif', NDSI.isel(time=5), list(NDSI.data_vars), compr='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAX, min, mean NDSI by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_ndsi(i):\n",
    "    dataset_clean, clean_mask = load_multi_clean(dc = dc, products = products , time = [start_date, end_date],\n",
    "                                                      lon = geographic_chunks[i]['longitude'],\n",
    "                                                      lat = geographic_chunks[i]['latitude'],\n",
    "                                                      measurements = measurements)\n",
    "    if dataset_clean != 0:\n",
    "        ndsi = (dataset_clean.green - dataset_clean.swir1) / (dataset_clean.green + dataset_clean.swir1)\n",
    "\n",
    "        ndsi_count = ndsi.count(dim=['time'])\n",
    "        ndsi_qual = np.rint((ndsi_count / len(ndsi['time']) * 100)).astype(np.uint8)\n",
    "        del dataset_clean\n",
    "        del clean_mask\n",
    "        #max\n",
    "        ndsi_max = ndsi.max(dim = ['time'])\n",
    "        ndsi_max.name = 'ndsi_max'\n",
    "        #min\n",
    "        ndsi_min = ndsi.min(dim = ['time'])\n",
    "        ndsi_min.name = 'ndsi_min'        \n",
    "        #mean\n",
    "        ndsi_mean = ndsi.mean(dim = ['time'])\n",
    "        ndsi_mean.name = 'ndsi_mean'        \n",
    "\n",
    "        # Append result\n",
    "        ndsi_max_chunks.append(ndsi_max.to_dataset(name='max'))\n",
    "        del ndsi_max\n",
    "        ndsi_min_chunks.append(ndsi_min.to_dataset(name='min'))\n",
    "        del ndsi_min\n",
    "        ndsi_mean_chunks.append(ndsi_mean.to_dataset(name='mean'))\n",
    "        del ndsi_mean\n",
    "        \n",
    "        ndsi_count_chunks.append(ndsi_count.to_dataset(name='count'))\n",
    "        del ndsi_count\n",
    "        ndsi_qual_chunks.append(ndsi_qual.to_dataset(name='qual'))\n",
    "        del ndsi_qual\n",
    "      \n",
    "        \n",
    "    else:\n",
    "        print(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multi_ndsi(i):\n",
    "    dataset_clean, clean_mask = load_multi_clean(dc = dc, products = products , time = [start_date, end_date],\n",
    "                                                      lon = geographic_chunks[i]['longitude'],\n",
    "                                                      lat = geographic_chunks[i]['latitude'],\n",
    "                                                      measurements = measurements)\n",
    "    if dataset_clean != 0:\n",
    "        swir1 = dataset_clean.swir1\n",
    "        nir = dataset_clean.nir\n",
    "        red = dataset_clean.red\n",
    "\n",
    "        ndsi_count = swir1.count(dim=['time'])\n",
    "        ndsi_qual = np.rint((ndsi_count / len(swir1['time']) * 100)).astype(np.uint8)\n",
    "        del dataset_clean\n",
    "        del clean_mask\n",
    "        #max\n",
    "        swir1_max = swir1.max(dim = ['time'])\n",
    "        swir1_max.name = 'swir1'\n",
    "        #max\n",
    "        nir_max = nir.max(dim = ['time'])\n",
    "        nir_max.name = 'nir'        \n",
    "        #mean\n",
    "        red_max = red.max(dim = ['time'])\n",
    "        red_max.name = 'red'        \n",
    "\n",
    "        # Append result\n",
    "        swir1_max_chunks.append(swir1_max.to_dataset(name='max'))\n",
    "        del swir1_max\n",
    "        nir_max_chunks.append(nir_max.to_dataset(name='min'))\n",
    "        del nir_max\n",
    "        red_max_chunks.append(red_max.to_dataset(name='mean'))\n",
    "        del red_max\n",
    "      \n",
    "        \n",
    "    else:\n",
    "        print(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-15 14:44:39.759999 | NDSI max, min, mean by month calculation started\n",
      "Processing 12/2000\n",
      "[####################] in 0:04:14.387555 - COMPLETED\n",
      "2024-04-15 14:49:11.435789 | NDSI monthly raw data calculated (done in 0:04:31.675169)\n",
      "2024-04-15 14:49:11.437465 | ALL DONE\n",
      "\n",
      "CPU times: user 3min 36s, sys: 1min 14s, total: 4min 51s\n",
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Create/Empty working directory\n",
    "if not os.path.exists(work_path):\n",
    "    os.makedirs(work_path)\n",
    "\n",
    "# Cut the geographic extents into chunks\n",
    "chunk_size = 0.5\n",
    "geographic_chunks = create_geographic_chunks(latitude=(min_lat, max_lat),\n",
    "                                             longitude=(min_lon, max_lon),\n",
    "                                             geographic_chunk_size=chunk_size)\n",
    "tot = len(geographic_chunks)\n",
    "\n",
    "printandlog('NDSI max, min, mean by month calculation started')\n",
    "start_time = datetime.now()\n",
    "years = range(start_year, end_year + 1)\n",
    "months = range(start_month, end_month + 1)\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        starty_time = datetime.now()\n",
    "        print('Processing %i/%i' % (month, year))\n",
    "\n",
    "        start_date = datetime.strptime(\"%i-%i-1\" % (year, month), '%Y-%m-%d')\n",
    "        # Need to find last days of a given month\n",
    "        end_date = datetime.strptime(\"%i-%i-%i\" % (year, month, calendar.monthrange(year, month)[1]), '%Y-%m-%d')\n",
    "\n",
    "        ndsi_max_chunks = []\n",
    "        ndsi_min_chunks = []\n",
    "        ndsi_mean_chunks = []\n",
    "        ndsi_count_chunks = []\n",
    "        ndsi_qual_chunks = []\n",
    "\n",
    "        bar_len = 20\n",
    "        \n",
    "        for i in range(tot): \n",
    "            multi_ndsi(i)\n",
    "            filled_len = round(bar_len * (i + 1) /  tot)\n",
    "            #\\r commande à Python de revenir à la ligne où {0} et {1} sont des indices \n",
    "            sys.stdout.write('\\r[{0}{1}] in {2}'\n",
    "                         .format('#' * filled_len, '-' * (bar_len - filled_len), datetime.now() - starty_time))\n",
    "        sys.stdout.write('\\r[{0}] in {1} - COMPLETED\\n'.format('#' * bar_len, datetime.now() - starty_time))\n",
    "        \n",
    "        if len (ndsi_max_chunks) > 0:\n",
    "            # Combine chunks\n",
    "            with np.errstate(divide='ignore'):\n",
    "                NDSI = combine_geographic_chunks(ndsi_max_chunks)\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_min_chunks))\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_mean_chunks)) \n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_count_chunks))\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_qual_chunks))\n",
    "\n",
    "            del ndsi_max_chunks\n",
    "            del ndsi_min_chunks\n",
    "            del ndsi_mean_chunks\n",
    "            del ndsi_count_chunks\n",
    "            del ndsi_qual_chunks\n",
    "            \n",
    "            NDSI = NDSI.where((Mask_lacs ==0))\n",
    "            NDSI = NDSI.where((Mask_Suisse ==1))\n",
    "\n",
    "            write_geotiff_from_xr('%s/NDSI_%s_%02d_l7.tif'% (work_path, year, month), NDSI, list(NDSI.data_vars), compr='')\n",
    "\n",
    "            \n",
    "printandlog('NDSI monthly raw data calculated', log_name, started = start_time)\n",
    "            \n",
    "            \n",
    "printandlog('ALL DONE\\n', log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAX, mean, min NDSI by month mix landsat-Sentinel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ndsi(i):\n",
    "    dict_dsc, dict_cm = load_lss2_clean_30(dc = dc,\n",
    "                                    products = products ,\n",
    "                                    time = [start_date, end_date],\n",
    "                                    lon = geographic_chunks[i]['longitude'],\n",
    "                                    lat = geographic_chunks[i]['latitude'],\n",
    "                                    measurements = measurements,\n",
    "                                    #resampl = 'up',\n",
    "                                    #dropna = True ,\n",
    "                                    valid_cats = [[1,2,4],[4,5,6,7,11]])\n",
    "    \n",
    "    dict_dsc['ls']['mask'] = dict_dsc['ls']['pixel_qa']\n",
    "    dict_dsc['ls'] = dict_dsc['ls'].drop(['pixel_qa'])\n",
    "    \n",
    "    dict_dsc['s2']['mask'] = dict_dsc['s2']['slc']\n",
    "    dict_dsc['s2'] = dict_dsc['s2'].drop(['slc']) \n",
    "    \n",
    "    da = xr.concat((dict_dsc['ls'],dict_dsc['s2']), data_vars=dict_dsc['s2'].data_vars , dim='time')\n",
    "    \n",
    "    ds = da.sortby('time')    \n",
    "    \n",
    "    if ds != 0:\n",
    "        ndsi = (ds.green - ds.swir1) / (ds.green + ds.swir1)\n",
    "\n",
    "        ndsi_count = ndsi.count(dim=['time'])\n",
    "        ndsi_qual = np.rint((ndsi_count / len(ndsi['time']) * 100)).astype(np.uint8)\n",
    "        del dict_dsc\n",
    "        del dict_cm\n",
    "        #max\n",
    "        ndsi_max = ndsi.max(dim = ['time'])\n",
    "        ndsi_max.name = 'ndsi_max'\n",
    "        #min\n",
    "        ndsi_min = ndsi.min(dim = ['time'])\n",
    "        ndsi_min.name = 'ndsi_min'        \n",
    "        #mean\n",
    "        ndsi_mean = ndsi.mean(dim = ['time'])\n",
    "        ndsi_mean.name = 'ndsi_mean'        \n",
    "\n",
    "        # Append result\n",
    "        ndsi_max_chunks.append(ndsi_max.to_dataset(name='max'))\n",
    "        del ndsi_max\n",
    "        ndsi_min_chunks.append(ndsi_min.to_dataset(name='min'))\n",
    "        del ndsi_min\n",
    "        ndsi_mean_chunks.append(ndsi_mean.to_dataset(name='mean'))\n",
    "        del ndsi_mean\n",
    "        \n",
    "        ndsi_count_chunks.append(ndsi_count.to_dataset(name='count'))\n",
    "        del ndsi_count\n",
    "        ndsi_qual_chunks.append(ndsi_qual.to_dataset(name='qual'))\n",
    "        del ndsi_qual\n",
    "      \n",
    "        \n",
    "    else:\n",
    "        print(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ndsi(i):\n",
    "    dict_dsc, dict_cm = load_lss2_clean_30(dc = dc,\n",
    "                                    products = products ,\n",
    "                                    time = [start_date, end_date],\n",
    "                                    lon = geographic_chunks[i]['longitude'],\n",
    "                                    lat = geographic_chunks[i]['latitude'],\n",
    "                                    measurements = measurements,\n",
    "                                    #resampl = 'up',\n",
    "                                    #dropna = True ,\n",
    "                                    valid_cats = [[1,2,4],[4,5,6,7,11]])\n",
    "    \n",
    "    dict_dsc['ls']['mask'] = dict_dsc['ls']['pixel_qa']\n",
    "    dict_dsc['ls'] = dict_dsc['ls'].drop(['pixel_qa'])\n",
    "    \n",
    "    dict_dsc['s2']['mask'] = dict_dsc['s2']['slc']\n",
    "    dict_dsc['s2'] = dict_dsc['s2'].drop(['slc']) \n",
    "    \n",
    "    da = xr.concat((dict_dsc['ls'],dict_dsc['s2']), data_vars=dict_dsc['s2'].data_vars , dim='time')\n",
    "    \n",
    "    ds = da.sortby('time')    \n",
    "    \n",
    "    if ds != 0:\n",
    "        swir1 = ds.swir1\n",
    "        nir = ds.nir\n",
    "        red = ds.red\n",
    "\n",
    "        ndsi_count = swir1.count(dim=['time'])\n",
    "        ndsi_qual = np.rint((ndsi_count / len(swir1['time']) * 100)).astype(np.uint8)\n",
    "        del dict_dsc\n",
    "        del dict_cm\n",
    "        #max\n",
    "        swir1_max = swir1.max(dim = ['time'])\n",
    "        swir1_max.name = 'swir1'\n",
    "        #max\n",
    "        nir_max = nir.max(dim = ['time'])\n",
    "        nir_max.name = 'nir'        \n",
    "        #mean\n",
    "        red_max = red.max(dim = ['time'])\n",
    "        red_max.name = 'red'        \n",
    "\n",
    "        # Append result\n",
    "        swir1_max_chunks.append(swir1_max.to_dataset(name='max'))\n",
    "        del swir1_max\n",
    "        nir_max_chunks.append(nir_max.to_dataset(name='min'))\n",
    "        del nir_max\n",
    "        red_max_chunks.append(red_max.to_dataset(name='mean'))\n",
    "        del red_max\n",
    "      \n",
    "        \n",
    "    else:\n",
    "        print(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create/Empty working directory\n",
    "if not os.path.exists(work_path):\n",
    "    os.makedirs(work_path)\n",
    "\n",
    "# Cut the geographic extents into chunks\n",
    "chunk_size = 0.5\n",
    "geographic_chunks = create_geographic_chunks(latitude=(min_lat, max_lat),\n",
    "                                             longitude=(min_lon, max_lon),\n",
    "                                             geographic_chunk_size=chunk_size)\n",
    "tot = len(geographic_chunks)\n",
    "\n",
    "printandlog('NDSI max, min, mean by month calculation started')\n",
    "start_time = datetime.now()\n",
    "years = range(start_year, end_year + 1)\n",
    "months = range(start_month, end_month + 1)\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        starty_time = datetime.now()\n",
    "        print('Processing %i/%i' % (month, year))\n",
    "\n",
    "        start_date = datetime.strptime(\"%i-%i-1\" % (year, month), '%Y-%m-%d')\n",
    "        # Need to find last days of a given month\n",
    "        end_date = datetime.strptime(\"%i-%i-%i\" % (year, month, calendar.monthrange(year, month)[1]), '%Y-%m-%d')\n",
    "\n",
    "        ndsi_max_chunks = []\n",
    "        ndsi_min_chunks = []\n",
    "        ndsi_mean_chunks = []\n",
    "        ndsi_count_chunks = []\n",
    "        ndsi_qual_chunks = []\n",
    "\n",
    "        bar_len = 20\n",
    "        \n",
    "        for i in range(tot): \n",
    "            multi_ndsi(i)\n",
    "            filled_len = round(bar_len * (i + 1) /  tot)\n",
    "            #\\r commande à Python de revenir à la ligne où {0} et {1} sont des indices \n",
    "            sys.stdout.write('\\r[{0}{1}] in {2}'\n",
    "                         .format('#' * filled_len, '-' * (bar_len - filled_len), datetime.now() - starty_time))\n",
    "        sys.stdout.write('\\r[{0}] in {1} - COMPLETED\\n'.format('#' * bar_len, datetime.now() - starty_time))\n",
    "        \n",
    "        if len (ndsi_max_chunks) > 0:\n",
    "            # Combine chunks\n",
    "            with np.errstate(divide='ignore'):\n",
    "                NDSI = combine_geographic_chunks(ndsi_max_chunks)\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_min_chunks))\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_mean_chunks)) \n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_count_chunks))\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_qual_chunks))\n",
    "\n",
    "            del ndsi_max_chunks\n",
    "            del ndsi_min_chunks\n",
    "            del ndsi_mean_chunks\n",
    "            del ndsi_count_chunks\n",
    "            del ndsi_qual_chunks\n",
    "            \n",
    "            #NDSI = NDSI.where((Mask_lacs ==0))\n",
    "            #NDSI = NDSI.where((Mask_Suisse ==1))\n",
    "\n",
    "            write_geotiff_from_xr('%s/NDSI_%s_%02d.tif'% (work_path, year, month), NDSI, list(NDSI.data_vars), compr='')\n",
    "\n",
    "            \n",
    "printandlog('NDSI monthly raw data calculated', log_name, started = start_time)\n",
    "            \n",
    "            \n",
    "printandlog('ALL DONE\\n', log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create/Empty working directory\n",
    "if not os.path.exists(work_path):\n",
    "    os.makedirs(work_path)\n",
    "\n",
    "# Cut the geographic extents into chunks\n",
    "chunk_size = 0.5\n",
    "geographic_chunks = create_geographic_chunks(latitude=(min_lat, max_lat),\n",
    "                                             longitude=(min_lon, max_lon),\n",
    "                                             geographic_chunk_size=chunk_size)\n",
    "tot = len(geographic_chunks)\n",
    "\n",
    "printandlog('NDSI max, min, mean by month calculation started')\n",
    "start_time = datetime.now()\n",
    "years = range(start_year, end_year + 1)\n",
    "months = range(start_month, end_month + 1)\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        starty_time = datetime.now()\n",
    "        print('Processing %i/%i' % (month, year))\n",
    "\n",
    "        start_date = datetime.strptime(\"%i-%i-1\" % (year, month), '%Y-%m-%d')\n",
    "        # Need to find last days of a given month\n",
    "        end_date = datetime.strptime(\"%i-%i-%i\" % (year, month, calendar.monthrange(year, month)[1]), '%Y-%m-%d')\n",
    "\n",
    "        swir1_max_chunks = []\n",
    "        nir_max_chunks = []\n",
    "        red_max_chunks = []\n",
    "\n",
    "\n",
    "        bar_len = 20\n",
    "        \n",
    "        for i in range(tot): \n",
    "            multi_ndsi(i)\n",
    "            filled_len = round(bar_len * (i + 1) /  tot)\n",
    "            #\\r commande à Python de revenir à la ligne où {0} et {1} sont des indices \n",
    "            sys.stdout.write('\\r[{0}{1}] in {2}'\n",
    "                         .format('#' * filled_len, '-' * (bar_len - filled_len), datetime.now() - starty_time))\n",
    "        sys.stdout.write('\\r[{0}] in {1} - COMPLETED\\n'.format('#' * bar_len, datetime.now() - starty_time))\n",
    "        \n",
    "        if len (swir1_max_chunks) > 0:\n",
    "            # Combine chunks\n",
    "            with np.errstate(divide='ignore'):\n",
    "                NDSI = combine_geographic_chunks(swir1_max_chunks)\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(nir_max_chunks)) \n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(red_max_chunks))\n",
    "\n",
    "            del swir1_max_chunks\n",
    "            del nir_max_chunks\n",
    "            del red_max_chunks\n",
    "\n",
    "            \n",
    "            #NDSI = NDSI.where((Mask_lacs ==0))\n",
    "            #NDSI = NDSI.where((Mask_Suisse ==1))\n",
    "\n",
    "            write_geotiff_from_xr('%s/NDSI_%s_%02d.tif'% (work_path, year, month), NDSI, list(NDSI.data_vars), compr='')\n",
    "\n",
    "            \n",
    "printandlog('NDSI monthly raw data calculated', log_name, started = start_time)\n",
    "            \n",
    "            \n",
    "printandlog('ALL DONE\\n', log_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max min mean by month with only sentinel-2 at 30m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ndsi(i):\n",
    "    dataset_clean, clean_mask = load_multi_clean_30(dc = dc, products = products , time = [start_date, end_date],\n",
    "                                                      lon = geographic_chunks[i]['longitude'],\n",
    "                                                      lat = geographic_chunks[i]['latitude'],\n",
    "                                                      valid_cats = [4,5,6,7,11],\n",
    "                                                    measurements= measurements,\n",
    "                                                   resolution =(0.0003407435000010011,0.0003407435000010325),\n",
    "                                                   resampling ={'*': 'cubic', 'slc': 'nearest'})\n",
    "    if dataset_clean != 0:\n",
    "        ndsi = (dataset_clean.green - dataset_clean.swir1) / (dataset_clean.green + dataset_clean.swir1)\n",
    "\n",
    "        ndsi_count = ndsi.count(dim=['time'])\n",
    "        ndsi_qual = np.rint((ndsi_count / len(ndsi['time']) * 100)).astype(np.uint8)\n",
    "        del dataset_clean\n",
    "        del clean_mask\n",
    "        #max\n",
    "        ndsi_max = ndsi.max(dim = ['time'])\n",
    "        ndsi_max.name = 'ndsi_max'\n",
    "        #min\n",
    "        ndsi_min = ndsi.min(dim = ['time'])\n",
    "        ndsi_min.name = 'ndsi_min'        \n",
    "        #mean\n",
    "        ndsi_mean = ndsi.mean(dim = ['time'])\n",
    "        ndsi_mean.name = 'ndsi_mean'        \n",
    "\n",
    "        # Append result\n",
    "        ndsi_max_chunks.append(ndsi_max.to_dataset(name='max'))\n",
    "        del ndsi_max\n",
    "        ndsi_min_chunks.append(ndsi_min.to_dataset(name='min'))\n",
    "        del ndsi_min\n",
    "        ndsi_mean_chunks.append(ndsi_mean.to_dataset(name='mean'))\n",
    "        del ndsi_mean\n",
    "        \n",
    "        ndsi_count_chunks.append(ndsi_count.to_dataset(name='count'))\n",
    "        del ndsi_count\n",
    "        ndsi_qual_chunks.append(ndsi_qual.to_dataset(name='qual'))\n",
    "        del ndsi_qual\n",
    "      \n",
    "        \n",
    "    else:\n",
    "        print(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create/Empty working directory\n",
    "if not os.path.exists(work_path):\n",
    "    os.makedirs(work_path)\n",
    "\n",
    "# Cut the geographic extents into chunks\n",
    "chunk_size = 0.5\n",
    "geographic_chunks = create_geographic_chunks(latitude=(min_lat, max_lat),\n",
    "                                             longitude=(min_lon, max_lon),\n",
    "                                             geographic_chunk_size=chunk_size)\n",
    "tot = len(geographic_chunks)\n",
    "\n",
    "printandlog('NDSI max, min, mean by month calculation started')\n",
    "start_time = datetime.now()\n",
    "years = range(start_year, end_year + 1)\n",
    "months = range(start_month, end_month + 1)\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        starty_time = datetime.now()\n",
    "        print('Processing %i/%i' % (month, year))\n",
    "\n",
    "        start_date = datetime.strptime(\"%i-%i-1\" % (year, month), '%Y-%m-%d')\n",
    "        # Need to find last days of a given month\n",
    "        end_date = datetime.strptime(\"%i-%i-%i\" % (year, month, calendar.monthrange(year, month)[1]), '%Y-%m-%d')\n",
    "\n",
    "        ndsi_max_chunks = []\n",
    "        ndsi_min_chunks = []\n",
    "        ndsi_mean_chunks = []\n",
    "        ndsi_count_chunks = []\n",
    "        ndsi_qual_chunks = []\n",
    "\n",
    "        bar_len = 20\n",
    "        \n",
    "        for i in range(tot): \n",
    "            multi_ndsi(i)\n",
    "            filled_len = round(bar_len * (i + 1) /  tot)\n",
    "            #\\r commande à Python de revenir à la ligne où {0} et {1} sont des indices \n",
    "            sys.stdout.write('\\r[{0}{1}] in {2}'\n",
    "                         .format('#' * filled_len, '-' * (bar_len - filled_len), datetime.now() - starty_time))\n",
    "        sys.stdout.write('\\r[{0}] in {1} - COMPLETED\\n'.format('#' * bar_len, datetime.now() - starty_time))\n",
    "        \n",
    "        if len (ndsi_max_chunks) > 0:\n",
    "            # Combine chunks\n",
    "            with np.errstate(divide='ignore'):\n",
    "                NDSI = combine_geographic_chunks(ndsi_max_chunks)\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_min_chunks))\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_mean_chunks)) \n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_count_chunks))\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_qual_chunks))\n",
    "\n",
    "            del ndsi_max_chunks\n",
    "            del ndsi_min_chunks\n",
    "            del ndsi_mean_chunks\n",
    "            del ndsi_count_chunks\n",
    "            del ndsi_qual_chunks\n",
    "            \n",
    "            #NDSI = NDSI.where((Mask_lacs ==0))\n",
    "            #NDSI = NDSI.where((Mask_Suisse ==1))\n",
    "\n",
    "            write_geotiff_from_xr('%s/NDSI_%s_%02d.tif'% (work_path, year, month), NDSI, list(NDSI.data_vars), compr='')\n",
    "\n",
    "            \n",
    "printandlog('NDSI monthly raw data calculated', log_name, started = start_time)\n",
    "            \n",
    "            \n",
    "printandlog('ALL DONE\\n', log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAX NDSI by month with a thr of 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndsi_threshold = 0.4\n",
    "def multi_ndsi(i):\n",
    "    dataset_clean, clean_mask = load_multi_clean(dc = dc, products = products , time = [start_date, end_date],\n",
    "                                                      lon = geographic_chunks[i]['longitude'],\n",
    "                                                      lat = geographic_chunks[i]['latitude'],\n",
    "                                                      measurements = measurements)\n",
    "    if dataset_clean != 0:\n",
    "        ndsi = (dataset_clean.green - dataset_clean.swir1) / (dataset_clean.green + dataset_clean.swir1)\n",
    "\n",
    "        ndsi_count = ndsi.count(dim=['time'])\n",
    "        ndsi_qual = np.rint((ndsi_count / len(ndsi['time']) * 100)).astype(np.uint8)\n",
    "        del dataset_clean\n",
    "        del clean_mask\n",
    "        ndsi_max = ndsi.max(dim = ['time'])\n",
    "        ndsi_max.name = 'ndsi_max'\n",
    "        #for a threshold of 0.4\n",
    "        #Here we want to keep nan as nan, 0 for NDSi < 0.4 and 1 for NDSI > 0.4\n",
    "        #This step is important for the time series analyses, we have to distinguish nan to 0 \n",
    "        ndsi_thr = ndsi_max > ndsi_threshold\n",
    "        ndsi_thr.astype(int)\n",
    "        #\n",
    "        ndsi_thr1 = ndsi_thr * ndsi_max\n",
    "        ndsi_thr2 = ndsi_thr1.notnull()\n",
    "        positive = ndsi_thr1 > 0\n",
    "        ndsi_thr3 = positive.fillna(0.).where(ndsi_thr2)\n",
    "        ndsi_thr3\n",
    "\n",
    "        # Append result\n",
    "        ndsi_max_chunks.append(ndsi_max.to_dataset(name='max'))\n",
    "        del ndsi_max\n",
    "        ndsi_count_chunks.append(ndsi_count.to_dataset(name='count'))\n",
    "        del ndsi_count\n",
    "        ndsi_qual_chunks.append(ndsi_qual.to_dataset(name='qual'))\n",
    "        del ndsi_qual\n",
    "        ndsi_thr_chunks.append(ndsi_thr3.to_dataset(name=\"snow\"))\n",
    "        del ndsi_thr\n",
    "        del ndsi_thr1\n",
    "        del ndsi_thr2\n",
    "        del ndsi_thr3\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create/Empty working directory\n",
    "if not os.path.exists(work_path):\n",
    "    os.makedirs(work_path)\n",
    "\n",
    "# Cut the geographic extents into chunks\n",
    "chunk_size = 0.5\n",
    "geographic_chunks = create_geographic_chunks(latitude=(min_lat, max_lat),\n",
    "                                             longitude=(min_lon, max_lon),\n",
    "                                             geographic_chunk_size=chunk_size)\n",
    "tot = len(geographic_chunks)\n",
    "\n",
    "print('NDSI max by month calculation started')\n",
    "start_time = datetime.now()\n",
    "years = range(start_year, end_year + 1)\n",
    "months = range(start_month, end_month + 1)\n",
    "\n",
    "for year in years:\n",
    "    for month in months:\n",
    "        starty_time = datetime.now()\n",
    "        print('Processing %i/%i' % (month, year))\n",
    "\n",
    "        start_date = datetime.strptime(\"%i-%i-1\" % (year, month), '%Y-%m-%d')\n",
    "        # Need to find last days of a given month\n",
    "        end_date = datetime.strptime(\"%i-%i-%i\" % (year, month, calendar.monthrange(year, month)[1]), '%Y-%m-%d')\n",
    "\n",
    "        ndsi_max_chunks = []\n",
    "        ndsi_count_chunks = []\n",
    "        ndsi_qual_chunks = []\n",
    "        ndsi_thr_chunks = []\n",
    "\n",
    "        bar_len = 20\n",
    "        \n",
    "#         for i, _ in enumerate(p.imap(multi_ndvi, range(tot))):\n",
    "#             filled_len = round(bar_len * (i + 1) / tot)\n",
    "#             sys.stdout.write('\\r[{0}{1}] in {2}'\n",
    "#                              .format('#' * filled_len, '-' * (bar_len - filled_len), datetime.now() - starty_time))\n",
    "#         sys.stdout.write('\\r[{0}] in {1} - COMPLETED\\n'.format('#' * bar_len, datetime.now() - starty_time))\n",
    "#         p.close()\n",
    "        \n",
    "        for i in range(tot): \n",
    "            multi_ndsi(i)\n",
    "            filled_len = round(bar_len * (i + 1) /  tot)\n",
    "            #\\r commande à Python de revenir à la ligne où {0} et {1} sont des indices \n",
    "            sys.stdout.write('\\r[{0}{1}] in {2}'\n",
    "                         .format('#' * filled_len, '-' * (bar_len - filled_len), datetime.now() - starty_time))\n",
    "            sys.stdout.write('\\r[{0}] in {1} - COMPLETED\\n'.format('#' * bar_len, datetime.now() - starty_time))\n",
    "        \n",
    "        if len (ndsi_max_chunks) > 0:\n",
    "            # Combine chunks\n",
    "            with np.errstate(divide='ignore'):\n",
    "                NDSI = combine_geographic_chunks(ndsi_max_chunks)    \n",
    "                #NDSI = NDSI.merge(combine_geographic_chunks(ndsi_count_chunks))\n",
    "                #NDSI = NDSI.merge(combine_geographic_chunks(ndsi_qual_chunks))\n",
    "                NDSI = NDSI.merge(combine_geographic_chunks(ndsi_thr_chunks))\n",
    "            del ndsi_max_chunks\n",
    "            del ndsi_count_chunks\n",
    "            del ndsi_qual_chunks\n",
    "            del ndsi_thr_chunks\n",
    "            NDSI.to_netcdf('%s/NDSI_%s_%02d.nc' % (work_path, year, month))\n",
    "\n",
    "            \n",
    "printandlog('NDSI monthly max calculated', log_name, started = start_time)\n",
    "            \n",
    "            \n",
    "printandlog('ALL DONE\\n', log_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAX, min, mean NDSI by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_ndsi(i):\n",
    "    dict_dsc, dict_cm = load_lss2_clean_30(dc = dc,\n",
    "                                    products = products ,\n",
    "                                    time = [start_date, end_date],\n",
    "                                    lon = geographic_chunks[i]['longitude'],\n",
    "                                    lat = geographic_chunks[i]['latitude'],\n",
    "                                    measurements = measurements,\n",
    "                                    #resampl = 'up',\n",
    "                                    #dropna = True ,\n",
    "                                    valid_cats = [[1,2,4],[4,5,6,7,11]])\n",
    "    \n",
    "    dict_dsc['ls']['mask'] = dict_dsc['ls']['pixel_qa']\n",
    "    dict_dsc['ls'] = dict_dsc['ls'].drop(['pixel_qa'])\n",
    "    \n",
    "    dict_dsc['s2']['mask'] = dict_dsc['s2']['slc']\n",
    "    dict_dsc['s2'] = dict_dsc['s2'].drop(['slc']) \n",
    "    \n",
    "    da = xr.concat((dict_dsc['ls'],dict_dsc['s2']), data_vars=dict_dsc['s2'].data_vars , dim='time')\n",
    "    \n",
    "    ds = da.sortby('time')   \n",
    "    \n",
    "    if ds != 0:\n",
    "        ndsi = (ds.green - ds.swir1) / (ds.green + ds.swir1)\n",
    "\n",
    "        #ndsi_count = ndsi.count(dim=['time'])\n",
    "        #ndsi_qual = np.rint((ndsi_count / len(ndsi['time']) * 100)).astype(np.uint8)\n",
    "        del dict_dsc\n",
    "        del dict_cm\n",
    "        #max\n",
    "        #ndsi_max = ndsi.max(dim = ['time'])\n",
    "        #ndsi_max.name = 'ndsi_max'\n",
    "        #min\n",
    "        #ndsi_min = ndsi.min(dim = ['time'])\n",
    "        #ndsi_min.name = 'ndsi_min'        \n",
    "        #mean\n",
    "        ndsi_mean = ndsi.mean(dim = ['time'])\n",
    "        ndsi_mean.name = 'ndsi_mean'        \n",
    "\n",
    "        # Append result\n",
    "        #ndsi_max_chunks.append(ndsi_max.to_dataset(name='max'))\n",
    "        #del ndsi_max\n",
    "        #ndsi_min_chunks.append(ndsi_min.to_dataset(name='min'))\n",
    "        #del ndsi_min\n",
    "        ndsi_mean_chunks.append(ndsi_mean.to_dataset(name='mean'))\n",
    "        del ndsi_mean\n",
    "        \n",
    "        #ndsi_count_chunks.append(ndsi_count.to_dataset(name='count'))\n",
    "        #del ndsi_count\n",
    "        #ndsi_qual_chunks.append(ndsi_qual.to_dataset(name='qual'))\n",
    "        #del ndsi_qual\n",
    "      \n",
    "        \n",
    "    else:\n",
    "        print(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create working directory if necessary\n",
    "if not os.path.exists(work_path):\n",
    "    os.makedirs(work_path)\n",
    "\n",
    "# Cut the geographic extents into chunks\n",
    "chunk_size = 0.1\n",
    "geographic_chunks = create_geographic_chunks(latitude=(min_lat, max_lat),\n",
    "                                             longitude=(min_lon, max_lon),\n",
    "                                             geographic_chunk_size=chunk_size)\n",
    "tot = len(geographic_chunks)\n",
    "\n",
    "printandlog('NDWI annual mean calculation started', log_name, reset = True)\n",
    "start_time = datetime.now()\n",
    "years = range(start_year, end_year + 1)\n",
    "\n",
    "for year_ind, year in enumerate(years):\n",
    "    starty_time = datetime.now()\n",
    "    printandlog('Processing year %i' % year, log_name)\n",
    "\n",
    "    start_date = datetime.strptime(\"%i-%i-1\" % (year, start_month), '%Y-%m-%d')\n",
    "    # Need to find last days of a given month\n",
    "    end_date = datetime.strptime(\"%i-%i-%i\" % (year, end_month, calendar.monthrange(year, end_month)[1]), '%Y-%m-%d')\n",
    "    \n",
    "    #ndsi_max_chunks = []\n",
    "    #ndsi_min_chunks = []\n",
    "    ndsi_mean_chunks = []\n",
    "    #ndsi_count_chunks = []\n",
    "    #ndsi_qual_chunks = []\n",
    "    \n",
    "    bar_len = 20\n",
    "\n",
    "    for i in range(tot): \n",
    "        multi_ndsi(i)\n",
    "        filled_len = round(bar_len * (i + 1) /  tot)\n",
    "        #\\r commande à Python de revenir à la ligne où {0} et {1} sont des indices \n",
    "        sys.stdout.write('\\r[{0}{1}] in {2}'\n",
    "                         .format('#' * filled_len, '-' * (bar_len - filled_len), datetime.now() - starty_time))\n",
    "    sys.stdout.write('\\r[{0}] in {1} - COMPLETED\\n'.format('#' * bar_len, datetime.now() - starty_time))\n",
    "        \n",
    "    if len (ndsi_mean_chunks) > 0:\n",
    "        # Combine chunks\n",
    "        with np.errstate(divide='ignore'):\n",
    "            NDSI = combine_geographic_chunks(ndsi_mean_chunks)\n",
    "            #NDSI = NDSI.merge(combine_geographic_chunks(ndsi_min_chunks))\n",
    "            #NDSI = NDSI.merge(combine_geographic_chunks(ndsi_mean_chunks)) \n",
    "            #NDSI = NDSI.merge(combine_geographic_chunks(ndsi_count_chunks))\n",
    "            #NDSI = NDSI.merge(combine_geographic_chunks(ndsi_qual_chunks))\n",
    "\n",
    "        #del ndsi_max_chunks\n",
    "        #del ndsi_min_chunks\n",
    "        del ndsi_mean_chunks\n",
    "        #del ndsi_count_chunks\n",
    "        #del ndsi_qual_chunks\n",
    "            \n",
    "        #NDSI = NDSI.where((Mask_lacs ==0))\n",
    "        #NDSI = NDSI.where((Mask_Suisse ==1))\n",
    "\n",
    "        write_geotiff_from_xr('%s/NDSI_%s_v4.tif'% (work_path, year), NDSI, list(NDSI.data_vars), compr='')\n",
    "\n",
    "            \n",
    "printandlog('NDSI annual raw data calculated', log_name, started = start_time)\n",
    "            \n",
    "            \n",
    "printandlog('ALL DONE\\n', log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format(Mask_lacs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NDSI = NDSI.where((Mask_lacs ==0))\n",
    "NDSI = NDSI.where((Mask_Suisse ==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
